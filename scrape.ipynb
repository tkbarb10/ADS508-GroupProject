{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to scrape lost data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting requests\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests)\n",
      "  Downloading charset_normalizer-3.4.1-cp313-cp313-win_amd64.whl.metadata (36 kB)\n",
      "Collecting idna<4,>=2.5 (from requests)\n",
      "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests)\n",
      "  Downloading urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests)\n",
      "  Downloading certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\n",
      "Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Downloading certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
      "Downloading charset_normalizer-3.4.1-cp313-cp313-win_amd64.whl (102 kB)\n",
      "Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "Downloading urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
      "Installing collected packages: urllib3, idna, charset-normalizer, certifi, requests\n",
      "Successfully installed certifi-2025.1.31 charset-normalizer-3.4.1 idna-3.10 requests-2.32.3 urllib3-2.3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: C:\\Users\\tkbar\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement io (from versions: none)\n",
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: C:\\Users\\tkbar\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n",
      "ERROR: No matching distribution found for io\n"
     ]
    }
   ],
   "source": [
    "!pip install requests\n",
    "!pip install io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting bs4\n",
      "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
      "Collecting beautifulsoup4 (from bs4)\n",
      "  Downloading beautifulsoup4-4.13.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4->bs4)\n",
      "  Downloading soupsieve-2.6-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typing-extensions>=4.0.0 (from beautifulsoup4->bs4)\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
      "Downloading beautifulsoup4-4.13.3-py3-none-any.whl (186 kB)\n",
      "Downloading soupsieve-2.6-py3-none-any.whl (36 kB)\n",
      "Downloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Installing collected packages: typing-extensions, soupsieve, beautifulsoup4, bs4\n",
      "Successfully installed beautifulsoup4-4.13.3 bs4-0.0.2 soupsieve-2.6 typing-extensions-4.12.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: C:\\Users\\tkbar\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pages found: 1\n",
      "Scraping page 1...\n",
      "  International Designator NORAD Catalog Number          Satellite Name  \\\n",
      "0                1960-009D                   52  ECHO 1 DEB [MYLAR OBJ]   \n",
      "1                1963-039C                  692                  VELA 1   \n",
      "2                1964-040A                  836       OPS 3662 (VELA 3)   \n",
      "3                1965-058A                 1458       OPS 6577 (VELA 5)   \n",
      "4                1965-058B                 1459       OPS 6564 (VELA 6)   \n",
      "\n",
      "  Source Launch Date Launch Site Object Type                   Last Data  \n",
      "0     US  1960-08-12       AFETR         DEB  1991-03-25T14:23:57.069312  \n",
      "1     US  1963-10-17       AFETR        PAY-  2024-11-06T17:09:09.800928  \n",
      "2     US  1964-07-17       AFETR        PAY-  2024-10-18T19:49:13.468512  \n",
      "3     US  1965-07-20       AFETR        PAY-  2025-01-24T09:01:13.708704  \n",
      "4     US  1965-07-20       AFETR        PAY-  2024-12-24T02:18:46.645344  \n"
     ]
    }
   ],
   "source": [
    "# Scrape code\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Base URL for lost satellites\n",
    "base_url = \"https://celestrak.org/satcat/lost.php\"\n",
    "\n",
    "# Initialize empty list to store data\n",
    "all_rows = []\n",
    "\n",
    "# Fetch the first page to determine pagination\n",
    "response = requests.get(base_url)\n",
    "if response.status_code != 200:\n",
    "    print(f\"Failed to retrieve data: {response.status_code}\")\n",
    "else:\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    # Find the total number of pages (if pagination exists)\n",
    "    pagination = soup.find(\"div\", class_=\"pagination\")\n",
    "    if pagination:\n",
    "        pages = [a.text for a in pagination.find_all(\"a\") if a.text.isdigit()]\n",
    "        total_pages = max(map(int, pages)) if pages else 1\n",
    "    else:\n",
    "        total_pages = 1  # If no pagination, assume one page\n",
    "\n",
    "    print(f\"Total pages found: {total_pages}\")\n",
    "\n",
    "    # Loop through all pages\n",
    "    for page in range(1, total_pages + 1):\n",
    "        print(f\"Scraping page {page}...\")\n",
    "        page_url = f\"{base_url}?page={page}\" if total_pages > 1 else base_url\n",
    "        response = requests.get(page_url)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to retrieve page {page}: {response.status_code}\")\n",
    "            continue\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        table = soup.find(\"table\")\n",
    "        \n",
    "        if table:\n",
    "            # Extract headers from first page only\n",
    "            if page == 1:\n",
    "                headers = [header.text.strip() for header in table.find_all(\"th\")]\n",
    "\n",
    "            # Extract data rows\n",
    "            for row in table.find_all(\"tr\")[1:]:  # Skip header row\n",
    "                cols = [col.text.strip() for col in row.find_all(\"td\")]\n",
    "                if cols:\n",
    "                    all_rows.append(cols)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(all_rows, columns=headers)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"CelesTrek/lost_objects.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
