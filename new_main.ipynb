{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import requests\n",
    "from pyathena import connect\n",
    "import io\n",
    "from bs4 import BeautifulSoup\n",
    "from scipy.stats import zscore\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "import re\n",
    "import os\n",
    "import sagemaker\n",
    "import s3fs\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "account_id = boto3.client(\"sts\").get_caller_identity().get(\"Account\")\n",
    "\n",
    "sm = boto3.Session().client(service_name=\"sagemaker\", region_name=region)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Data Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Retrieving European Space Agency (ESA) Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "# API Endpoint\n",
    "esa_api_url = \"https://discosweb.esoc.esa.int/api/objects\"\n",
    "\n",
    "# Your Personal Access Token\n",
    "token = os.getenv('esa_token')\n",
    "\n",
    "# Set up authentication headers\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {token}\",\n",
    "    \"DiscosWeb-Api-Version\": \"2\",\n",
    "    \"Accept\": \"application/vnd.api+json\"\n",
    "}\n",
    "\n",
    "# Query Parameters (Retrieving all object attributes)\n",
    "params = {\n",
    "    \"page[size]\": 100,  # Max allowed per page\n",
    "}\n",
    "\n",
    "# List to store all object records\n",
    "all_data = []\n",
    "page = 1  # Start with page 1\n",
    "\n",
    "with tqdm(desc=\"Fetching Data\", unit=\" records\", smoothing=0.1) as pbar:\n",
    "    while True:\n",
    "        params[\"page[number]\"] = page  # Set current page number\n",
    "        response = requests.get(esa_api_url, headers=headers, params=params)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            \n",
    "            if \"data\" in data and data[\"data\"]:\n",
    "                all_data.extend(data[\"data\"])  # Store raw data\n",
    "                pbar.update(len(data[\"data\"]))  # Update progress bar\n",
    "            else:\n",
    "                break  # Stop if no more data\n",
    "            \n",
    "            page += 1  # Move to next page\n",
    "        else:\n",
    "            print(f\"❌ Error: {response.status_code}, {response.text}\")\n",
    "            break\n",
    "\n",
    "# Extract all object attributes\n",
    "esa_df = pd.DataFrame([obj[\"attributes\"] for obj in all_data])\n",
    "\n",
    "# Save as Parquet for efficiency\n",
    "# esa_df.to_parquet(\"data/raw data/full_esa_data.parquet\", index=False)\n",
    "\n",
    "print(f\"✅ Retrieved {len(satcat_df)} records and saved as Parquet!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Scrape Lost Object Data from CelesTrek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base URL for lost satellites\n",
    "base_url = \"https://celestrak.org/satcat/lost.php\"\n",
    "\n",
    "# Initialize empty list to store data\n",
    "all_rows = []\n",
    "\n",
    "# Fetch the first page to determine pagination\n",
    "response = requests.get(base_url)\n",
    "if response.status_code != 200:\n",
    "    print(f\"Failed to retrieve data: {response.status_code}\")\n",
    "else:\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    # Find the total number of pages (if pagination exists)\n",
    "    pagination = soup.find(\"div\", class_=\"pagination\")\n",
    "    if pagination:\n",
    "        pages = [a.text for a in pagination.find_all(\"a\") if a.text.isdigit()]\n",
    "        total_pages = max(map(int, pages)) if pages else 1\n",
    "    else:\n",
    "        total_pages = 1  # If no pagination, assume one page\n",
    "\n",
    "    print(f\"Total pages found: {total_pages}\")\n",
    "\n",
    "    # Loop through all pages\n",
    "    for page in range(1, total_pages + 1):\n",
    "        print(f\"Scraping page {page}...\")\n",
    "        page_url = f\"{base_url}?page={page}\" if total_pages > 1 else base_url\n",
    "        response = requests.get(page_url)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to retrieve page {page}: {response.status_code}\")\n",
    "            continue\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        table = soup.find(\"table\")\n",
    "        \n",
    "        if table:\n",
    "            # Extract headers from first page only\n",
    "            if page == 1:\n",
    "                headers = [header.text.strip() for header in table.find_all(\"th\")]\n",
    "\n",
    "            # Extract data rows\n",
    "            for row in table.find_all(\"tr\")[1:]:  # Skip header row\n",
    "                cols = [col.text.strip() for col in row.find_all(\"td\")]\n",
    "                if cols:\n",
    "                    all_rows.append(cols)\n",
    "\n",
    "# Convert to DataFrame\n",
    "lost_object_satcat_df = pd.DataFrame(all_rows, columns=headers)\n",
    "\n",
    "#Ingest to folder\n",
    "lost_object_satcat_df.to_csv(\"data/raw data/lost_objects.csv\", index = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Retrieve Low Earth Object data from Space Track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Space-Track login credentials\n",
    "USERNAME = os.getenv('user')\n",
    "PASSWORD = os.getenv('pass')\n",
    "\n",
    "# Space-Track API URL for querying LEO satellites (sample query from website)\n",
    "url = \"https://www.space-track.org/basicspacedata/query/class/satcat/PERIOD/<128/DECAY/null-val/CURRENT/Y/\"\n",
    "\n",
    "# Create a session for authentication\n",
    "session = requests.Session()\n",
    "\n",
    "# Authenticate with space-track.org\n",
    "login_url = \"https://www.space-track.org/ajaxauth/login\"\n",
    "login_data = {\"identity\": USERNAME, \"password\": PASSWORD}\n",
    "session.post(login_url, data=login_data)\n",
    "\n",
    "# Fetch JSON data\n",
    "response = session.get(url)\n",
    "\n",
    "# Ensure request was successful\n",
    "if response.status_code == 200:\n",
    "    json_data = response.json()  # Convert response to JSON format\n",
    "\n",
    "    # Convert JSON to Pandas DataFrame\n",
    "    leo_satcat_df = pd.DataFrame(json_data)\n",
    "\n",
    "    #leo_satcat_df.to_csv(\"data/raw data/leo_objects.csv\", index = False)\n",
    "\n",
    "else:\n",
    "    print(\"Failed to retrieve data. Check credentials or API access.\")\n",
    "    print(response.text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Ingest into s3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp \"data/raw data/full_esa_data.parquet\" \"s3://{bucket}/data/raw_data/full_esa_folder/\"\n",
    "!aws s3 cp \"data/raw data/full_satcat.csv\" \"s3://{bucket}/data/raw_data/satcat_folder/\"\n",
    "!aws s3 cp \"data/raw data/leo_objects.csv\" \"s3://{bucket}/data/raw_data/leo_objects_folder/\"\n",
    "!aws s3 cp \"data/raw data/lost_objects.csv\" \"s3://{bucket}/data/raw_data/lost_objects_folder/\"\n",
    "!aws s3 cp \"data/raw data/simplified_collision_data.csv\" \"s3://{bucket}/data/raw_data/simplified_collision_folder/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Full ESA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('Data to use/full_esa_data.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Can immediately see that some features will need data types changed such as satno or predDecayDate**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Check outliers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of numeric columns to plot\n",
    "numeric_cols = ['mass', 'width', 'height', 'depth', 'diameter', 'span', 'xSectMax', 'xSectMin','xSectAvg', \n",
    "                'cataloguedFragments', 'onOrbitCataloguedFragments']\n",
    "\n",
    "# Set up the matplotlib grid\n",
    "fig, axes = plt.subplots(4, 3, figsize=(12, 8))  # 2x2 grid\n",
    "\n",
    "# Flatten the axes array for easy iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Loop through each numeric column and plot\n",
    "for i, col in enumerate(numeric_cols):\n",
    "    sns.boxplot(y=col, data=df, ax=axes[i])\n",
    "    axes[i].set_title(f'{col}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import textwrap\n",
    "\n",
    "plt.figure(figsize = (8, 6))\n",
    "\n",
    "ax = df['objectClass'].value_counts().plot(kind='bar', title=\"Object Class Breakdown\")\n",
    "plt.xticks(rotation=45, ha='right', fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['firstEpoch'] = pd.to_datetime(df['firstEpoch'])\n",
    "\n",
    "df['firstEpoch'].value_counts().sort_index().plot(x = 'firstEpoch', kind = 'line')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Missing Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp 's3://sagemaker-us-east-1-266917422334/data/lost_objects.csv' ./data-folder/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data-folder/lost_objects.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Launch Site'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "df[['Launch Site', 'Source']].groupby('Source').count().plot(kind = 'bar')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Launch Date'] = pd.to_datetime(df['Launch Date'])\n",
    "launch_counts = df.groupby(df['Launch Date'].dt.to_period(\"Y\"))['International Designator'].count()\n",
    "\n",
    "launch_counts.index = launch_counts.index.astype(str)\n",
    "\n",
    "plt.figure(figsize = (15,5))\n",
    "\n",
    "plt.plot(launch_counts.index, launch_counts.values, marker = 'o', linestyle = '-')\n",
    "\n",
    "plt.xticks(rotation = 45)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Object Type'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining all into one DataFrame\n",
    "combined_df = pd.concat([\n",
    "    esa_df_small,\n",
    "    full_satcat_df_small,\n",
    "    lost_objects_df_small,\n",
    "    loe_objects_df_small\n",
    "], ignore_index=True)\n",
    "\n",
    "# Dropping the rows with missing object_type\n",
    "combined_df = combined_df.dropna(subset=['object_type'])\n",
    "\n",
    "print(\"Combined dataset shape:\", combined_df.shape)\n",
    "combined_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ploting object type distribution\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.countplot(data=combined_df, x='object_type', order=combined_df['object_type'].value_counts().index)\n",
    "plt.title(\"Object Type Distribution\")\n",
    "plt.xlabel(\"Object Type\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ploting object frequency by year\n",
    "combined_df['timestamp'] = pd.to_datetime(combined_df['timestamp'], errors='coerce')\n",
    "\n",
    "# Drop rows with invalid dates (NaT)\n",
    "combined_df = combined_df.dropna(subset=['timestamp'])\n",
    "\n",
    "combined_df['year'] = combined_df['timestamp'].dt.year\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.histplot(data=combined_df, x='year', hue='object_type', multiple='stack', bins=50)\n",
    "plt.title(\"Objects Over Time by Type\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Clean/Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## ESA Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esa_df = pd.read_parquet(f\"s3://{bucket}/data/raw_data/full_esa_folder/full_esa_data.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns with too many missing values or not useful for modeling\n",
    "\n",
    "columns_to_drop = ['vimpelId', 'satno', 'predDecayDate', 'active', 'mission', 'shape', 'mass', 'width', 'height', 'depth', 'xSectMax', 'xSectMin']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns and check missing values again\n",
    "\n",
    "esa_df.drop(columns = columns_to_drop, inplace = True)\n",
    "\n",
    "esa_df.isna().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to Date time\n",
    "\n",
    "esa_df['firstEpoch'] = pd.to_datetime(esa_df['firstEpoch'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lot of rows are all NA or 0, but have positive values for first Epoch, so removing those\n",
    "\n",
    "exclude_col = ['firstEpoch', 'objectClass']\n",
    "\n",
    "# Drop rows where all values (except the exclude_col) are NA or 0\n",
    "esa_df = esa_df[~((esa_df.drop(columns=exclude_col).isna()) | (esa_df.drop(columns=exclude_col) == 0)).all(axis=1)]\n",
    "\n",
    "esa_df.isna().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name and object class are still necessary, so if both are null then remove, otherwise keep and deal with the other missing data as needed\n",
    "\n",
    "esa_df_cleaned = esa_df[~esa_df[['cosparId', 'name']].isna().all(axis = 1)]\n",
    "\n",
    "esa_df_cleaned.isna().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save this esa dataframe separately to graph object class for Goal # 1 before further processing for modeling\n",
    "\n",
    "esa_df_cleaned.to_parquet(\"data/processed data/cleaned_esa_goal1.parquet\", index = False)\n",
    "esa_df_cleaned.to_parquet(f\"s3://{bucket}/data/processed_data/esa_folder/cleaned_esa_goal1.parquet\", index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy and collapse objectClass category into 4 values\n",
    "\n",
    "esa_df_modeling = esa_df_cleaned.copy()\n",
    "\n",
    "esa_df_modeling['objectClass'] = esa_df_modeling['objectClass'].apply(lambda x: 'Debris' if 'Debris' in x else x)\n",
    "esa_df_modeling.loc[esa_df_modeling['objectClass'].str.contains('Mission'), 'objectClass'] = 'Debris'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esa_df_modeling.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the numerical features and save\n",
    "\n",
    "standard_cols = ['diameter', 'span', 'xSectAvg', 'cataloguedFragments', 'onOrbitCataloguedFragments']\n",
    "\n",
    "esa_df_modeling[['diameter', 'span', 'xSectAvg']] = esa_df_modeling[['diameter', 'span', 'xSectAvg']].apply(lambda x: x.fillna(x.median()))\n",
    "\n",
    "esa_df_modeling[standard_cols] = esa_df_modeling[standard_cols].apply(zscore)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns for consistency\n",
    "\n",
    "esa_df_modeling.rename(columns = {\n",
    "    'cosparId': 'object_id',\n",
    "    'name': 'object_name',\n",
    "    'objectClass': 'object_type',\n",
    "    'firstEpoch': 'launch_date'\n",
    "},\n",
    "                      inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize column names\n",
    "esa_df_modeling.columns = esa_df_modeling.columns.str.strip().str.lower().str.replace(\" \", \"_\")\n",
    "\n",
    "print(\"Updated Column Names:\\n\", esa_df_modeling.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esa_df_modeling.to_parquet(\"data/processed data/esa_df_modeling.parquet\", index = False)\n",
    "esa_df_modeling.to_parquet(f\"s3://{bucket}/data/processed_data/esa_folder/esa_df_modeling.parquet\", index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Satcat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "satcat_df = pd.read_csv(f\"s3://{bucket}/data/raw_data/satcat_folder/full_satcat.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "\n",
    "missing_values = satcat_df.isnull().sum()\n",
    "print(\"Missing Values:\\n\", missing_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check unique values per column\n",
    "\n",
    "unique_counts = satcat_df.nunique().sort_values()\n",
    "print(\"Unique Values per Column:\\n\", unique_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop uneccessary columns\n",
    "\n",
    "columns_to_drop = ['Unnamed: 0', 'DATA_STATUS_CODE', 'RCS', 'OPS_STATUS_CODE', 'DATA_STATUS_CODE']\n",
    "satcat_df = satcat_df.drop(columns = columns_to_drop)\n",
    "\n",
    "# Only concerned with objects in orbit around Earth, so remove all others\n",
    "\n",
    "satcat_df = satcat_df[satcat_df['ORBIT_CENTER'] == 'EA']\n",
    "satcat_df = satcat_df.drop(columns = 'ORBIT_CENTER')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change names of orbit type to be consistent with others\n",
    "\n",
    "satcat_df['OBJECT_TYPE'] = satcat_df['OBJECT_TYPE'].replace({\n",
    "    'DEB': 'DEBRIS',\n",
    "    'PAY': 'PAYLOAD',\n",
    "    'R/B': 'ROCKET BODY',\n",
    "    'UNK': 'UNKNOWN'\n",
    "})\n",
    "\n",
    "satcat_df['NORAD_CAT_ID'] = satcat_df['NORAD_CAT_ID'].astype('str')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in some of the NA values for the distance and location metrics.  If orbit type is IMP or LAN that means they're no longer in orbit\n",
    "\n",
    "satcat_df.loc[(satcat_df['ORBIT_TYPE'] == 'IMP') | (satcat_df['ORBIT_TYPE'] == 'LAN'), ['PERIOD', 'INCLINATION', 'APOGEE', 'PERIGEE']] = 0\n",
    "\n",
    "# If still in orbit, impute the median (to adjust for outliers)\n",
    "\n",
    "cols = ['PERIOD', 'INCLINATION', 'APOGEE', 'PERIGEE']\n",
    "satcat_df[cols] = satcat_df[cols].fillna(satcat_df[satcat_df['ORBIT_TYPE'] == 'ORB'][cols].median())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new columns called Has Decayed so Decay Dates can be kept\n",
    "\n",
    "satcat_df['HAS_DECAYED'] = satcat_df['DECAY_DATE'].notna().astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize column names\n",
    "\n",
    "satcat_df.columns = satcat_df.columns.str.strip().str.lower().str.replace(\" \", \"_\")\n",
    "\n",
    "print(\"Updated Column Names:\\n\", satcat_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date columns \n",
    "\n",
    "date_columns = [\"launch_date\", 'decay_date']  \n",
    "for col in date_columns:\n",
    "    if col in satcat_df.columns:\n",
    "        satcat_df[col] = pd.to_datetime(satcat_df[col], errors=\"coerce\")\n",
    "\n",
    "# Convert categorical columns\n",
    "\n",
    "categorical_cols = satcat_df.select_dtypes(include=[\"object\"]).columns\n",
    "for col in categorical_cols:\n",
    "    satcat_df[col] = satcat_df[col].astype(\"category\")\n",
    "\n",
    "# Check data types after conversion\n",
    "satcat_df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "\n",
    "satcat_df.to_parquet(\"data/processed data/satcat_df.parquet\", index = False)\n",
    "satcat_df.to_parquet(f\"s3://{bucket}/data/processed_data/satcat_folder/satcat_df.parquet\", index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Leo Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leo_df = pd.read_csv(f\"s3://{bucket}/data/raw_data/leo_objects_folder/leo_objects.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns that are repeat columns or unecessary for modeling\n",
    "\n",
    "columns_to_drop = ['Unnamed: 0', 'DECAY', 'COMMENT', 'COMMENTCODE', 'FILE', 'LAUNCH_YEAR', 'LAUNCH_PIECE', 'LAUNCH_NUM', 'RCSVALUE', \n",
    "                   'OBJECT_NUMBER', 'OBJECT_ID', 'OBJECT_NAME', 'CURRENT']\n",
    "\n",
    "leo_df = leo_df.drop(columns = columns_to_drop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change norad cat id from int to string and change launch to date value\n",
    "\n",
    "leo_df['NORAD_CAT_ID'] = leo_df['NORAD_CAT_ID'].astype('str')\n",
    "leo_df['LAUNCH'] = pd.to_datetime(leo_df['LAUNCH'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NA of RCS SIZE with distribution of corresponding object type\n",
    "\n",
    "leo_df[(leo_df['OBJECT_TYPE'] == 'UNKNOWN') & (leo_df['RCS_SIZE'].notna())]['RCS_SIZE'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize numerical columns\n",
    "\n",
    "leo_df_standardize = leo_df.copy()\n",
    "\n",
    "numerical_cols = ['PERIOD', 'INCLINATION', 'APOGEE', 'PERIGEE']\n",
    "\n",
    "leo_df_standardize[numerical_cols] = leo_df_standardize[numerical_cols].apply(zscore)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical columns\n",
    "\n",
    "categorical_cols = leo_df_standardize.select_dtypes(include=[\"object\"]).columns\n",
    "for col in categorical_cols:\n",
    "    leo_df_standardize[col] = leo_df_standardize[col].astype(\"category\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns for consistency\n",
    "\n",
    "leo_df_standardize.rename(columns = {\n",
    "    'INTLDES': 'object_id',\n",
    "    'SATNAME': 'object_name',\n",
    "    'SITE': 'launch_site',\n",
    "    'LAUNCH': 'launch_date'\n",
    "},\n",
    "             inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize column names\n",
    "\n",
    "leo_df_standardize.columns = leo_df_standardize.columns.str.strip().str.lower().str.replace(\" \", \"_\")\n",
    "\n",
    "print(\"Updated Column Names:\\n\", leo_df_standardize.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "\n",
    "leo_df_standardize.to_parquet(\"data/processed data/leo_objects.parquet\", index = False)\n",
    "leo_df_standardize.to_parquet(f\"s3://{bucket}/data/processed_data/leo_objects_folder/leo_objects.parquet\", index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Collision Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collision_df = pd.read_csv(f\"s3://{bucket}/data/raw_data/simplified_collision_folder/simplified_collision_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change data types\n",
    "\n",
    "collision_df[['NORAD_CAT_ID_1', 'NORAD_CAT_ID_2']] = collision_df[['NORAD_CAT_ID_1', 'NORAD_CAT_ID_2']].astype('category')\n",
    "\n",
    "collision_df['TCA'] = pd.to_datetime(collision_df['TCA'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize numerical columns\n",
    "\n",
    "num_cols = ['DSE_1', 'DSE_2', 'TCA_RANGE', 'TCA_RELATIVE_SPEED', 'DILUTION']\n",
    "collision_df[num_cols] = collision_df[num_cols].apply(zscore)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract activity status from object and create new column\n",
    "\n",
    "def extract_status(obj_name):\n",
    "    \"\"\"Extracts activity status and cleans the object name.\"\"\"\n",
    "    obj_name = str(obj_name)  # Ensure string type for parsing\n",
    "    is_active = 1 if \"[+]\" in obj_name else 0\n",
    "    cleaned_name = re.sub(r\"[\\[\\]\\+\\-]\", \"\", obj_name).strip()  # Remove [+] and [-]\n",
    "    return cleaned_name, is_active\n",
    "\n",
    "# Apply extraction function to both object columns\n",
    "collision_df[['OBJECT_NAME_1_Clean', 'OBJECT_1_Active']] = collision_df['OBJECT_NAME_1'].apply(lambda x: pd.Series(extract_status(x)))\n",
    "collision_df[['OBJECT_NAME_2_Clean', 'OBJECT_2_Active']] = collision_df['OBJECT_NAME_2'].apply(lambda x: pd.Series(extract_status(x)))\n",
    "\n",
    "# Create Activity_Status column based on activity status\n",
    "collision_df['Activity_Status'] = collision_df['OBJECT_1_Active'] + (collision_df['OBJECT_2_Active'] * 2)\n",
    "\n",
    "# Drop original object columns and rename cleaned ones\n",
    "collision_df = collision_df.drop(columns=['OBJECT_NAME_1', 'OBJECT_NAME_2'])\n",
    "collision_df = collision_df.rename(columns={'OBJECT_NAME_1_Clean': 'OBJECT_NAME_1', 'OBJECT_NAME_2_Clean': 'OBJECT_NAME_2'})\n",
    "\n",
    "# Standardize column names\n",
    "collision_df.columns = collision_df.columns.str.strip().str.lower().str.replace(\" \", \"_\")\n",
    "\n",
    "print(\"Updated Column Names:\\n\", collision_df.columns)\n",
    "\n",
    "# Change activity status to category\n",
    "\n",
    "collision_df['activity_status'] = collision_df['activity_status'].astype('category')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "\n",
    "collision_df.to_parquet(\"data/processed data/collision_data.parquet\", index=False)\n",
    "collision_df.to_parquet(f\"s3://{bucket}/data/processed_data/collision_data_folder/collision_data.parquet\", index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Lost Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lost_df = pd.read_csv(f\"s3://{bucket}/data/raw_data/lost_objects_folder/lost_objects.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop last row, artifact from scraping data\n",
    "\n",
    "lost_df = lost_df.iloc[:-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change data types appropriately and rename columns for consistency\n",
    "\n",
    "lost_df['NORAD Catalog Number'] = lost_df['NORAD Catalog Number'].astype('category')\n",
    "lost_df.rename(columns = {'NORAD Catalog Number': 'NORAD_CAT_ID', 'International Designator': 'object_id'}, inplace = True)\n",
    "lost_df['Last Data'] = pd.to_datetime(lost_df['Last Data'], format='ISO8601', errors='coerce')\n",
    "lost_df['Launch Date'] = pd.to_datetime(lost_df['Launch Date'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change value names of object type column\n",
    "\n",
    "lost_df['Object Type'] = lost_df['Object Type'].replace({\n",
    "    'DEB': 'DEBRIS',\n",
    "    'PAY-': 'PAYLOAD',\n",
    "    'PAY+': 'PAYLOAD',\n",
    "    'R/B': 'ROCKET BODY',\n",
    "    'UNK': 'UNKNOWN'\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize column names\n",
    "lost_df.columns = lost_df.columns.str.strip().str.lower().str.replace(\" \", \"_\")\n",
    "\n",
    "print(\"Updated Column Names:\\n\", lost_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "\n",
    "lost_df.to_parquet(\"data/processed data/lost_objects.parquet\", index = False)\n",
    "lost_df.to_parquet(f\"s3://{bucket}/data/processed_data/lost_object_folder/lost_objects.parquet\", index = False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Athena Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Create variable paths and store them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esa_path = \"s3://{}/data/processed_data/esa_folder/\".format(bucket)\n",
    "leo_path = \"s3://{}/data/processed_data/leo_objects_folder/\".format(bucket)\n",
    "lost_path = \"s3://{}/data/processed_data/lost_object_folder/\".format(bucket)\n",
    "satcat_path = \"s3://{}/data/processed_data/satcat_folder/\".format(bucket)\n",
    "collision_path = \"s3://{}/data/processed_data/collision_data_folder/\".format(bucket)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store esa_path\n",
    "%store leo_path\n",
    "%store lost_path\n",
    "%store satcat_path\n",
    "%store collision_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Create tables names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name1 = \"esa\"\n",
    "table_name2 = 'leo'\n",
    "table_name3 = 'lost_objects'\n",
    "table_name4 = 'satcat'\n",
    "table_name5 = 'collision'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Retrieve dataframes from directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create filesystem object\n",
    "\n",
    "fs = s3fs.S3FileSystem()\n",
    "\n",
    "# Retrieve all files paths\n",
    "\n",
    "parquet_paths = fs.find(f\"{bucket}/data/processed_data/\")\n",
    "\n",
    "# Loop through and add to a list\n",
    "\n",
    "dataframes = [pd.read_parquet(f\"s3://{path}\") for path in parquet_paths]\n",
    "\n",
    "# Save each df to own variable\n",
    "\n",
    "collision_df = dataframes[0]\n",
    "esa_df = dataframes[2]\n",
    "leo_df = dataframes[3]\n",
    "lost_df = dataframes[4]\n",
    "satcat_df = dataframes[5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Create staging directory path and initiate connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_staging_dir = \"s3://{}/athena/staging\".format(bucket)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store s3_staging_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = connect(region_name = region, s3_staging_dir = s3_staging_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Check current databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_sql(\"SHOW DATABASES\", conn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to project database\n",
    "\n",
    "project_conn = connect(region_name = region, s3_staging_dir = s3_staging_dir, schema_name = 'space_project')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Helper functions to generate table creation statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_dtype_to_athena(dtype):\n",
    "    dtype = str(dtype)\n",
    "    if 'int' in dtype:\n",
    "        return 'int'\n",
    "    elif 'float' in dtype:\n",
    "        return 'double'\n",
    "    elif 'bool' in dtype:\n",
    "        return 'boolean'\n",
    "    elif 'datetime' in dtype:\n",
    "        return 'timestamp'\n",
    "    else:\n",
    "        return 'string'  # default to string for object, category, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_athena_columns(df):\n",
    "    cols = []\n",
    "    for col in df.columns:\n",
    "        athena_type = map_dtype_to_athena(df[col].dtype)\n",
    "        cols.append(f\"    {col} {athena_type}\")\n",
    "    return \",\\n\".join(cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_athena_table_statement(table_name, s3_path, df, database_name='space_project'):\n",
    "    columns = generate_athena_columns(df)\n",
    "    statement = f\"\"\"CREATE EXTERNAL TABLE IF NOT EXISTS {database_name}.{table_name} (\n",
    "{columns}\n",
    ")\n",
    "STORED AS PARQUET\n",
    "LOCATION '{s3_path}'\"\"\"\n",
    "    return statement\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Create table statement then test to make sure connection went through"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esa_statement = create_athena_table_statement(table_name1, esa_path, esa_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_sql(esa_statement, project_conn)\n",
    "\n",
    "pd.read_sql('SELECT * FROM esa LIMIT 5', project_conn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Create other tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collision_statement = create_athena_table_statement(table_name5, collision_path, collision_df)\n",
    "leo_statement = create_athena_table_statement(table_name2, leo_path, leo_df)\n",
    "lost_statement = create_athena_table_statement(table_name3, lost_path, lost_df)\n",
    "satcat_statement = create_athena_table_statement(table_name4, satcat_path, satcat_df)\n",
    "\n",
    "pd.read_sql(collision_statement, conn)\n",
    "pd.read_sql(leo_statement, conn)\n",
    "pd.read_sql(lost_statement, conn)\n",
    "pd.read_sql(satcat_statement, conn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Check all tables were created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check all tables are there\n",
    "\n",
    "pd.read_sql(\"SHOW TABLES\", project_conn)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
