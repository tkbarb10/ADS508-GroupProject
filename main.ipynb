{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import requests\n",
    "import io\n",
    "from bs4 import BeautifulSoup\n",
    "from scipy.stats import zscore\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieving European Space Agency (ESA) Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "# API Endpoint\n",
    "esa_api_url = \"https://discosweb.esoc.esa.int/api/objects\"\n",
    "\n",
    "# Your Personal Access Token\n",
    "token = os.getenv('esa_token')\n",
    "\n",
    "# Set up authentication headers\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {token}\",\n",
    "    \"DiscosWeb-Api-Version\": \"2\",\n",
    "    \"Accept\": \"application/vnd.api+json\"\n",
    "}\n",
    "\n",
    "# Query Parameters (Retrieving all object attributes)\n",
    "params = {\n",
    "    \"page[size]\": 100,  # Max allowed per page\n",
    "}\n",
    "\n",
    "# List to store all object records\n",
    "all_data = []\n",
    "page = 1  # Start with page 1\n",
    "\n",
    "with tqdm(desc=\"Fetching Data\", unit=\" records\", smoothing=0.1) as pbar:\n",
    "    while True:\n",
    "        params[\"page[number]\"] = page  # Set current page number\n",
    "        response = requests.get(esa_api_url, headers=headers, params=params)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            \n",
    "            if \"data\" in data and data[\"data\"]:\n",
    "                all_data.extend(data[\"data\"])  # Store raw data\n",
    "                pbar.update(len(data[\"data\"]))  # Update progress bar\n",
    "            else:\n",
    "                break  # Stop if no more data\n",
    "            \n",
    "            page += 1  # Move to next page\n",
    "        else:\n",
    "            print(f\"❌ Error: {response.status_code}, {response.text}\")\n",
    "            break\n",
    "\n",
    "# Extract all object attributes\n",
    "esa_df = pd.DataFrame([obj[\"attributes\"] for obj in all_data])\n",
    "\n",
    "# Save as Parquet for efficiency\n",
    "# esa_df.to_parquet(\"Data to use/full_esa_data.parquet\", index=False)\n",
    "\n",
    "print(f\"✅ Retrieved {len(df)} records and saved as Parquet!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape Lost Object Data from CelesTrek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base URL for lost satellites\n",
    "base_url = \"https://celestrak.org/satcat/lost.php\"\n",
    "\n",
    "# Initialize empty list to store data\n",
    "all_rows = []\n",
    "\n",
    "# Fetch the first page to determine pagination\n",
    "response = requests.get(base_url)\n",
    "if response.status_code != 200:\n",
    "    print(f\"Failed to retrieve data: {response.status_code}\")\n",
    "else:\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    # Find the total number of pages (if pagination exists)\n",
    "    pagination = soup.find(\"div\", class_=\"pagination\")\n",
    "    if pagination:\n",
    "        pages = [a.text for a in pagination.find_all(\"a\") if a.text.isdigit()]\n",
    "        total_pages = max(map(int, pages)) if pages else 1\n",
    "    else:\n",
    "        total_pages = 1  # If no pagination, assume one page\n",
    "\n",
    "    print(f\"Total pages found: {total_pages}\")\n",
    "\n",
    "    # Loop through all pages\n",
    "    for page in range(1, total_pages + 1):\n",
    "        print(f\"Scraping page {page}...\")\n",
    "        page_url = f\"{base_url}?page={page}\" if total_pages > 1 else base_url\n",
    "        response = requests.get(page_url)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to retrieve page {page}: {response.status_code}\")\n",
    "            continue\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        table = soup.find(\"table\")\n",
    "        \n",
    "        if table:\n",
    "            # Extract headers from first page only\n",
    "            if page == 1:\n",
    "                headers = [header.text.strip() for header in table.find_all(\"th\")]\n",
    "\n",
    "            # Extract data rows\n",
    "            for row in table.find_all(\"tr\")[1:]:  # Skip header row\n",
    "                cols = [col.text.strip() for col in row.find_all(\"td\")]\n",
    "                if cols:\n",
    "                    all_rows.append(cols)\n",
    "\n",
    "# Convert to DataFrame\n",
    "lost_object_df = pd.DataFrame(all_rows, columns=headers)\n",
    "\n",
    "#Ingest to folder\n",
    "lost_object_df.to_csv(\"Data to use/lost_objects.csv\", index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve Low Earth Object data from Space Track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Space-Track login credentials\n",
    "USERNAME = os.getenv('user')\n",
    "PASSWORD = os.getenv('pass')\n",
    "\n",
    "# Space-Track API URL for querying LEO satellites (sample query from website)\n",
    "url = \"https://www.space-track.org/basicspacedata/query/class/satcat/PERIOD/<128/DECAY/null-val/CURRENT/Y/\"\n",
    "\n",
    "# Create a session for authentication\n",
    "session = requests.Session()\n",
    "\n",
    "# Authenticate with space-track.org\n",
    "login_url = \"https://www.space-track.org/ajaxauth/login\"\n",
    "login_data = {\"identity\": USERNAME, \"password\": PASSWORD}\n",
    "session.post(login_url, data=login_data)\n",
    "\n",
    "# Fetch JSON data\n",
    "response = session.get(url)\n",
    "\n",
    "# Ensure request was successful\n",
    "if response.status_code == 200:\n",
    "    json_data = response.json()  # Convert response to JSON format\n",
    "\n",
    "    # Convert JSON to Pandas DataFrame\n",
    "    leo_df = pd.DataFrame(json_data)\n",
    "\n",
    "    #leo_df.to_csv(\"Data to use/leo_objects.csv\", index = False)\n",
    "\n",
    "else:\n",
    "    print(\"Failed to retrieve data. Check credentials or API access.\")\n",
    "    print(response.text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
